{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8ce06a5",
   "metadata": {},
   "source": [
    "# OWL-ViT Object Detection Notebook\n",
    "\n",
    "This notebook demonstrates how to run and visualize object detections on high-resolution satellite imagery using the `OwlViTPipeline` helper class.  \n",
    "\n",
    "**Structure:**\n",
    "\n",
    "1. **Imports & Initialization**  \n",
    "   Import the pipeline class, create an instance for the chosen experiment (e.g. `baseline`), and configure any resize parameters.\n",
    "\n",
    "2. **Load Data**  \n",
    "   - Load & resize the input image.  \n",
    "   - Load the textual prompts used for zero-shot detection.\n",
    "\n",
    "3. **Run Experiments**  \n",
    "   - **Multi-threshold run**: `run_threshold_experiments([…])` to save visualizations and collect metrics across several confidence cutoffs.  \n",
    "   - **Single-threshold run**: `run_single_experiment(…)` for targeted analysis.  \n",
    "\n",
    "4. **Visualization Helpers**  \n",
    "   - **`show_only_prompt(results, prompt)`**: Plot detections for a specific prompt from pre-computed results.  \n",
    "   - **`run_and_show_prompt(threshold, prompt)`**: Convenience one-liner for inference + prompt-specific plotting.  \n",
    "\n",
    "5. **Export & Display Metrics**  \n",
    "   - **GeoJSON export**: `run_and_save_geojson(threshold)` to save all detections as a GeoJSON file.  \n",
    "   - **Metrics table**: `display_metrics()` to render a styled HTML table of detection counts and scores.  \n",
    "\n",
    "Feel free to change the `experiment_name` or `resize_size` when instantiating the pipeline, and to tweak thresholds or prompt strings below.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33d6eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "tools_loc = Path.cwd().resolve().parents[2]\n",
    "sys.path.insert(0, str(tools_loc))\n",
    "              \n",
    "from tools.owlvit_utils import OwlViTPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4987f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment configuration\n",
    "experiment_name      = 'baseline'       # e.g., 'baseline', 'tiling', etc.\n",
    "image_filename       = '<input_image_filename>'  # e.g., 'site_image.png'\n",
    "prompts_file         = '<prompts_file>'          # e.g., 'prompts.txt'\n",
    "resize_size          = (<width>, <height>)       # e.g., (1024, 1024)\n",
    "thresholds           = [<threshold_1>, <threshold_2>, <threshold_3>, <threshold_4>]  # e.g., [0.001, 0.002, ...]\n",
    "\n",
    "\n",
    "# Instantiate pipeline\n",
    "pipeline = OwlViTPipeline(\n",
    "    experiment_name=experiment_name,\n",
    "    resize_size=resize_size\n",
    ")\n",
    "\n",
    "# Load original image & prompts\n",
    "pipeline.load_image(image_filename)\n",
    "pipeline.load_prompts(prompts_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e000f50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pipeline.prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c83afa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# run detection at multiple thresholds (saves visuals and metrics)\n",
    "metrics = pipeline.run_threshold_experiments(thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f4d552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over thresholds\n",
    "for thresh in thresholds:\n",
    "\n",
    "    # the raw results dict:\n",
    "    inputs = pipeline.processor(\n",
    "        text    = pipeline.prompts,\n",
    "        images  = pipeline.image_resized,\n",
    "        return_tensors = \"pt\"\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        outputs = pipeline.model(**inputs)\n",
    "    results = pipeline.processor.post_process_object_detection(\n",
    "        outputs,\n",
    "        target_sizes=torch.tensor([[pipeline.image_full.height,\n",
    "                                    pipeline.image_full.width]]),\n",
    "        threshold=thresh\n",
    "    )[0]\n",
    "    \n",
    "    png_path = pipeline.save_visualisation(\n",
    "        results,\n",
    "        threshold=thresh\n",
    "    )\n",
    "    print(\"Saved visualization to:\", png_path)\n",
    "    \n",
    "    # Save the per-threshold metrics JSON\n",
    "    json_path = pipeline.save_metrics(\n",
    "        results,\n",
    "        threshold=thresh\n",
    "    )\n",
    "    print(\"Saved metrics to:\", json_path)\n",
    "    \n",
    "    # save geojson\n",
    "    geojson_path = pipeline.run_and_save_geojson(thresh)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27e99d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run inference at threshold and display only detections for the specified prompt\n",
    "for thresh in thresholds:\n",
    "    pipeline.run_and_show_prompt(\n",
    "        threshold=thresh,\n",
    "        target_prompt=\"<some_prompt>\"\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "owlvit-archaeo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
