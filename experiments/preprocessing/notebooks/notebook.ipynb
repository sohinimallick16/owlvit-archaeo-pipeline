{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3ceed04",
   "metadata": {},
   "source": [
    "\n",
    "# Pre-processing Experiment\n",
    "\n",
    "This notebook evaluates OWL-ViT detections after applying image pre-processing (histogram equalization, autocontrast, contrast enhancement, unsharp mask) to boost feature visibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6e10c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "tools_loc = Path.cwd().resolve().parents[2]\n",
    "sys.path.insert(0, str(tools_loc))\n",
    "              \n",
    "from tools.owlvit_utils   import OwlViTPipeline\n",
    "from tools.preprocess_utils import PreprocessUtils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6c415d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment configuration\n",
    "experiment_name      = 'preprocessing'\n",
    "image_filename       = '<input_image_filename>'  # e.g., 'site_image.png'\n",
    "prompts_file         = '<prompts_file>'          # e.g., 'prompts.txt'\n",
    "resize_size          = (<width>, <height>)       # e.g., (1024, 1024)\n",
    "thresholds           = [<threshold_1>, <threshold_2>, <threshold_3>, <threshold_4>]  # e.g., [0.001, 0.002, ...]\n",
    "\n",
    "# Instantiate pipeline\n",
    "pipeline = OwlViTPipeline(\n",
    "    experiment_name=experiment_name,\n",
    "    resize_size=resize_size\n",
    ")\n",
    "\n",
    "# Load original image & prompts\n",
    "pipeline.load_image(image_filename, image_name=image_name)\n",
    "pipeline.load_prompts(prompts_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c692404a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pipeline.prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fe5b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose which steps to apply\n",
    "apply_equalize       = True\n",
    "apply_autocontrast   = True\n",
    "contrast_factor      = 1.8\n",
    "apply_unsharp        = True\n",
    "\n",
    "# Run the pipeline-wide preprocess\n",
    "pre_img = PreprocessUtils.preprocess_pipeline(\n",
    "    pipeline.image_full,\n",
    "    equalize       = apply_equalize,\n",
    "    autocontrast   = apply_autocontrast,\n",
    "    contrast_factor= contrast_factor,\n",
    "    unsharp        = apply_unsharp\n",
    ")\n",
    "\n",
    "# Visual check\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "ax.imshow(pre_img)\n",
    "ax.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Override pipeline.image_resized with preprocessed image\n",
    "pipeline.image_resized = pre_img.resize(pipeline.resize_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf60dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over thresholds\n",
    "for thresh in thresholds:\n",
    "\n",
    "    # the raw results dict:\n",
    "    inputs = pipeline.processor(\n",
    "        text    = pipeline.prompts,\n",
    "        images  = pipeline.image_resized,\n",
    "        return_tensors = \"pt\"\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        outputs = pipeline.model(**inputs)\n",
    "    results = pipeline.processor.post_process_object_detection(\n",
    "        outputs,\n",
    "        target_sizes=torch.tensor([[pipeline.image_full.height,\n",
    "                                    pipeline.image_full.width]]),\n",
    "        threshold=thresh\n",
    "    )[0]\n",
    "    \n",
    "    png_path = pipeline.save_visualisation(\n",
    "        results,\n",
    "        threshold=thresh\n",
    "    )\n",
    "    print(\"Saved visualization to:\", png_path)\n",
    "    \n",
    "    # Save the per-threshold metrics JSON\n",
    "    json_path = pipeline.save_metrics(\n",
    "        results,\n",
    "        threshold=thresh,\n",
    "        suffix='tiling'\n",
    "    )\n",
    "    print(\"Saved metrics to:\", json_path)\n",
    "    \n",
    "    # save geojson\n",
    "    geojson_path = pipeline.run_and_save_geojson(thresh)\n",
    "    \n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "owlvit-archaeo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
